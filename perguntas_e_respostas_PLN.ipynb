{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvrJefVqFHdq"
      },
      "source": [
        "### Instala√ß√£o e leitura\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whiLeqdQK0Ry",
        "outputId": "9599737f-79f6-4dbc-9f8f-3d31405dd5ab",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pypdf>=3.0.0\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.0.0) (3.19.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.0.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.0.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.0.0) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.0.0) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, pypdf, PyMuPDF, faiss-cpu\n",
            "Successfully installed PyMuPDF-1.26.4 faiss-cpu-1.12.0 pypdf-6.0.0 python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF python-docx sentence-transformers faiss-cpu \"pypdf>=3.0.0\" \"transformers>=4.0.0\" \"torch>=2.0.0\" requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Script final para um sistema de Q&A com RAG, com melhorias avan√ßadas de extra√ß√£o:\n",
        "- Para PDFs: PyMuPDF com chunking baseado em senten√ßas para maior foco.\n",
        "- Para DOCX: python-docx com extra√ß√£o de contexto de tabelas para m√°xima precis√£o.\n",
        "- Consulta aos 3 modelos de QA Extrativos exigidos.\n",
        "\"\"\"\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import faiss\n",
        "import time\n",
        "import fitz  # PyMuPDF\n",
        "import docx # python-docx\n",
        "import re    # Para divis√£o de senten√ßas\n",
        "\n",
        "# NOVOS IMPORTS PARA A CORRE√á√ÉO DO DOCX\n",
        "from docx.oxml.table import CT_Tbl\n",
        "from docx.oxml.text.paragraph import CT_P\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n"
      ],
      "metadata": {
        "id": "CYKf4tYuc7e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CAMINHO_DO_PDF = \"doencas_respiratorias_cronicas.pdf\"\n",
        "CAMINHO_DO_DOCX = \"DICIONARIO_DE_DADOS.docx\"\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    API_TOKEN = userdata.get('HF_TOKEN')\n",
        "    if API_TOKEN is None: raise ValueError(\"Token n√£o encontrado\")\n",
        "    print(\"‚úÖ Token da Hugging Face carregado com sucesso!\")\n",
        "except (ImportError, KeyError, ValueError):\n",
        "    API_TOKEN = \"\" # INSIRA SEU TOKEN DA HUGGING FACE AQUI\n",
        "    if not API_TOKEN:\n",
        "        print(\"üõë ERRO: Token da Hugging Face n√£o configurado. Insira-o na vari√°vel API_TOKEN.\")\n",
        "\n",
        "# --- DICION√ÅRIO DE 3 MODELOS DE QA (EXTRATIVOS E LEVES) ---\n",
        "MODELOS_QA = {\n",
        "    \"mDeBERTa-v3-base-squad2\": \"https://api-inference.huggingface.co/models/timpal0l/mdeberta-v3-base-squad2\",\n",
        "    \"Google-bert\": \"https://api-inference.huggingface.co/models/google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
        "    \"RoBERTa-base-squad2\": \"https://api-inference.huggingface.co/models/deepset/roberta-base-squad2\"\n",
        "}\n",
        "\n",
        "HEADERS = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnMkI81VgIMm",
        "outputId": "c918e6a3-6bfa-4fe3-cdc6-b918ebab6d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Token da Hugging Face carregado com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_qa_api(payload, model_url):\n",
        "    model_name = model_url.split(\"/\")[-1]\n",
        "    max_retries = 3\n",
        "    wait_time = 15\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(model_url, headers=HEADERS, json=payload)\n",
        "            if response.status_code == 503:\n",
        "                print(f\"   ‚è≥ Modelo '{model_name}' est√° carregando... aguardando {wait_time}s\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"   üõë ERRO na chamada da API para '{model_name}': {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "    return {\"error\": f\"Modelo '{model_name}' n√£o respondeu.\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "kXeDSmUXgMtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processar_pdf_com_sentencas(caminho_arquivo):\n",
        "    \"\"\"Extrai texto de PDF, dividindo em senten√ßas e agrupando-as em chunks focados.\"\"\"\n",
        "    print(\"1/4 - Processando PDF com PyMuPDF (chunking por senten√ßas)...\")\n",
        "    doc = fitz.open(caminho_arquivo)\n",
        "    texto_completo = \"\".join([page.get_text(\"text\") for page in doc])\n",
        "    texto_completo = re.sub(r'\\s*\\n\\s*', ' ', texto_completo)\n",
        "    sentencas = re.split(r'(?<=[.!?])\\s+', texto_completo)\n",
        "\n",
        "    chunks_com_metadata = []\n",
        "    sentencas_por_chunk = 4\n",
        "    overlap = 1\n",
        "\n",
        "    for i in range(0, len(sentencas), sentencas_por_chunk - overlap):\n",
        "        grupo_sentencas = sentencas[i : i + sentencas_por_chunk]\n",
        "        chunk_texto = \" \".join(grupo_sentencas).strip()\n",
        "        if len(chunk_texto.split()) > 10:\n",
        "            chunks_com_metadata.append({\"text\": chunk_texto, \"metadata\": {\"page\": \"N/A\"}})\n",
        "\n",
        "    print(f\"‚úÖ PDF processado. {len(chunks_com_metadata)} chunks de senten√ßas criados.\")\n",
        "    return chunks_com_metadata\n",
        "\n",
        "def processar_docx_com_contexto_tabela(caminho_arquivo):\n",
        "    \"\"\"Extrai dados de tabelas de um DOCX, adicionando o nome da tabela como contexto.\"\"\"\n",
        "    print(\"1/4 - Processando DOCX com python-docx (com contexto de tabela)...\")\n",
        "    document = docx.Document(caminho_arquivo)\n",
        "    chunks_com_metadata = []\n",
        "\n",
        "    # Itera sobre os elementos do corpo do documento (par√°grafos e tabelas)\n",
        "    for i, block in enumerate(document.element.body):\n",
        "\n",
        "\n",
        "        if not isinstance(block, CT_Tbl):\n",
        "            continue\n",
        "\n",
        "\n",
        "        table = docx.table.Table(block, document)\n",
        "        contexto_tabela = \"Contexto n√£o identificado\"\n",
        "\n",
        "        # Procura por um par√°grafo imediatamente antes da tabela para usar como t√≠tulo\n",
        "        if i > 0 and isinstance(document.element.body[i-1], CT_P):\n",
        "            paragrafo_anterior = docx.text.paragraph.Paragraph(document.element.body[i-1], document)\n",
        "            if paragrafo_anterior.text.strip():\n",
        "                texto_paragrafo = \" \".join(paragrafo_anterior.text.strip().split())\n",
        "                match = re.search(r'LFCES\\d+,\\s*(\\w+)', texto_paragrafo)\n",
        "                if match:\n",
        "                    contexto_tabela = match.group(1)\n",
        "                else:\n",
        "                    contexto_tabela = texto_paragrafo\n",
        "\n",
        "        for j, row in enumerate(table.rows):\n",
        "            if j == 0: continue\n",
        "\n",
        "            try:\n",
        "                nome_campo = row.cells[0].text.strip()\n",
        "                descricao = row.cells[8].text.strip()\n",
        "                dominios = row.cells[9].text.strip()\n",
        "\n",
        "                if nome_campo and descricao:\n",
        "                    sentenca = f\"Na tabela '{contexto_tabela}', o campo '{nome_campo}' √© descrito como: '{descricao}'.\"\n",
        "                    if dominios:\n",
        "                        sentenca += f\" Seus dom√≠nios ou valores poss√≠veis s√£o: '{dominios}'.\"\n",
        "\n",
        "                    chunks_com_metadata.append({\"text\": sentenca, \"metadata\": {\"page\": \"N/A\"}})\n",
        "            except IndexError:\n",
        "                continue\n",
        "\n",
        "    print(f\"‚úÖ DOCX processado. {len(chunks_com_metadata)} chunks (senten√ßas com contexto) criados.\")\n",
        "    return chunks_com_metadata\n",
        "\n",
        "def indexar_documento(caminho_documento):\n",
        "    \"\"\"Fun√ß√£o unificada para processar e indexar qualquer tipo de documento suportado.\"\"\"\n",
        "    print(f\"\\n--- INICIANDO INDEXA√á√ÉO PARA: {os.path.basename(caminho_documento)} ---\")\n",
        "    if not os.path.exists(caminho_documento):\n",
        "        print(f\"üõë ERRO: Arquivo n√£o encontrado.\")\n",
        "        return None, None, None\n",
        "\n",
        "    if caminho_documento.lower().endswith('.pdf'):\n",
        "        chunks_com_metadata = processar_pdf_com_sentencas(caminho_documento)\n",
        "    elif caminho_documento.lower().endswith('.docx'):\n",
        "        chunks_com_metadata = processar_docx_com_contexto_tabela(caminho_documento)\n",
        "    else:\n",
        "        print(f\"üõë ERRO: Formato de arquivo n√£o suportado.\")\n",
        "        return None, None, None\n",
        "\n",
        "    if not chunks_com_metadata:\n",
        "        print(\"üõë Nenhuma informa√ß√£o v√°lida foi extra√≠da do documento.\")\n",
        "        return None, None, None\n",
        "\n",
        "    textos_para_embeddar = [chunk['text'] for chunk in chunks_com_metadata]\n",
        "\n",
        "    print(\"2/4 - Gerando embeddings...\")\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embeddings = embedding_model.encode(textos_para_embeddar, show_progress_bar=True)\n",
        "\n",
        "    print(\"3/4 - Criando √≠ndice FAISS...\")\n",
        "    faiss_index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    faiss_index.add(np.array(embeddings, dtype=np.float32))\n",
        "\n",
        "    print(f\"‚úÖ √çndice FAISS criado com {faiss_index.ntotal} vetores.\")\n",
        "    print(\"--- FASE DE INDEXA√á√ÉO CONCLU√çDA ---\")\n",
        "    return faiss_index, chunks_com_metadata, embedding_model\n"
      ],
      "metadata": {
        "id": "_G2ody56gOyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def executar_qa(faiss_index, chunks, embedding_model, cross_encoder_model, lista_de_perguntas, nome_documento):\n",
        "    if not all([faiss_index, chunks, embedding_model, cross_encoder_model]):\n",
        "        print(f\"\\nProcesso interrompido para '{nome_documento}' por erro na indexa√ß√£o.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n\\n--- INICIANDO CONSULTA PARA: {nome_documento} ---\")\n",
        "    for pergunta in lista_de_perguntas:\n",
        "        start_time = time.time()\n",
        "        print(f\"\\n\\n=================================================\")\n",
        "        print(f'‚ùì Pergunta: \"{pergunta}\"')\n",
        "        print(f\"=================================================\")\n",
        "\n",
        "        print(\"1. Buscando e re-classificando chunks...\")\n",
        "        query_embedding = embedding_model.encode([pergunta])\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "        k_retrieval = 15\n",
        "        _, indices = faiss_index.search(np.array(query_embedding, dtype=np.float32), k_retrieval)\n",
        "        retrieved_chunks_initial = [chunks[i] for i in indices[0]]\n",
        "        cross_encoder_input = [[pergunta, chunk['text']] for chunk in retrieved_chunks_initial]\n",
        "        scores = cross_encoder_model.predict(cross_encoder_input, show_progress_bar=False)\n",
        "        chunk_score_pairs = sorted(zip(retrieved_chunks_initial, scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        k_final = 1\n",
        "        reranked_chunks = [pair[0] for pair in chunk_score_pairs[:k_final] if pair[1] > -5.0]\n",
        "\n",
        "        if not reranked_chunks:\n",
        "            print(\"‚ùå N√£o foram encontrados chunks relevantes.\")\n",
        "            continue\n",
        "\n",
        "        contexto_final = \"\\n---\\n\".join([chunk['text'] for chunk in reranked_chunks])\n",
        "        fontes = sorted(list(set([chunk['metadata']['page'] for chunk in reranked_chunks])))\n",
        "        print(f\"‚úÖ Contexto final selecionado (fontes: {fontes}).\")\n",
        "\n",
        "        payload = {\"inputs\": {\"question\": pergunta, \"context\": contexto_final}}\n",
        "        print(\"\\n2. Consultando modelos de IA...\")\n",
        "        for nome_modelo, url_modelo in MODELOS_QA.items():\n",
        "            print(f\"\\n--- Consultando o modelo: {nome_modelo} ---\")\n",
        "            resultado = query_qa_api(payload, url_modelo)\n",
        "            if 'answer' in resultado and resultado['answer'] is not None:\n",
        "                print(f\"‚úÖ Resposta: {resultado['answer']}\")\n",
        "                print(f\"   Confian√ßa (Score): {resultado.get('score', 0):.4f}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Erro ou resposta n√£o encontrada: {resultado}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"\\n(Tempo total para esta pergunta: {end_time - start_time:.2f} segundos)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wDY3VDyTgUOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nCarregando modelo Cross-Encoder para re-ranking...\")\n",
        "    cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "    print(\"‚úÖ Cross-Encoder carregado.\")\n",
        "\n",
        "    # --- PROCESSO PARA O ARQUIVO PDF ---\n",
        "    faiss_index_pdf, chunks_pdf, embedding_model_pdf = indexar_documento(CAMINHO_DO_PDF)\n",
        "    if faiss_index_pdf:\n",
        "        lista_de_perguntas_pdf = [\n",
        "            \"Estou sentindo falta de ar, tossindo e com dor no peito. Qual doen√ßa possui esses sintomas?\",\n",
        "            \"Quais s√£o os sintomas da rinite al√©rgica?\",\n",
        "            \"Como posso determinar se tenho alguma doen√ßa respirat√≥ria cr√¥nica?\"\n",
        "        ]\n",
        "        executar_qa(faiss_index_pdf, chunks_pdf, embedding_model_pdf, cross_encoder_model, lista_de_perguntas_pdf, os.path.basename(CAMINHO_DO_PDF))\n",
        "\n",
        "    # --- PROCESSO PARA O ARQUIVO DOCX ---\n",
        "    faiss_index_docx, chunks_docx, embedding_model_docx = indexar_documento(CAMINHO_DO_DOCX)\n",
        "    if faiss_index_docx:\n",
        "        lista_de_perguntas_docx = [\n",
        "            \"Qual a descri√ß√£o e os dom√≠nios do campo STATUSMOV na tabela de estabelecimentos de sa√∫de?\",\n",
        "            \"Qual a descri√ß√£o do campo NSLAQCDURA na tabela de quimioterapia e radioterapia?\",\n",
        "            \"Qual a descri√ß√£o da tabela LFCES008?\"\n",
        "        ]\n",
        "        executar_qa(faiss_index_docx, chunks_docx, embedding_model_docx, cross_encoder_model, lista_de_perguntas_docx, os.path.basename(CAMINHO_DO_DOCX))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeQVRsxEgYGd",
        "outputId": "8c57b4d4-d6d4-4246-bdbb-d918e951c8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Carregando modelo Cross-Encoder para re-ranking...\n",
            "‚úÖ Cross-Encoder carregado.\n",
            "\n",
            "--- INICIANDO INDEXA√á√ÉO PARA: doencas_respiratorias_cronicas.pdf ---\n",
            "üõë ERRO: Arquivo n√£o encontrado.\n",
            "\n",
            "--- INICIANDO INDEXA√á√ÉO PARA: DICIONARIO_DE_DADOS.docx ---\n",
            "üõë ERRO: Arquivo n√£o encontrado.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}